Maximizing-Performance-by-EfficientNet-B3

This repository contains experiments aimed at maximizing binary classification performance (real vs. fake images) using EfficientNet-B3. Each script applies a single enhancement technique (e.g., AutoAugment, MixUp, DropPath, Cosine LR, EMA, MC Dropout), making it easy to compare improvements against the baseline.

Original repo: kkallongminho/Maximizing-Performance-by-EfficientNet-B3

â¸»

ğŸ“ Project Structure

Maximizing-Performance-by-EfficientNet-B3/
â”œâ”€ BASELINE/
â”‚  â””â”€ baseline.py            # Baseline using torchvision EfficientNet-B3
â”œâ”€ autoaugement/
â”‚  â””â”€ autoaugement.py        # AutoAugment (ImageNet policy)
â”œâ”€ cosine/
â”‚  â””â”€ cosine.py              # Cosine Annealing learning rate scheduler
â”œâ”€ droppath/
â”‚  â””â”€ droppath.py            # DropPath (Stochastic Depth)
â”œâ”€ ema/
â”‚  â””â”€ ema.py                 # Exponential Moving Average (EMA)
â”œâ”€ mcdropout/
â”‚  â””â”€ mcdropout.py           # Monte Carlo Dropout (uncertainty estimation)
â”œâ”€ mixup/
â”‚  â””â”€ mixup.py               # MixUp data augmentation
â””â”€ Result of the experiment.png  # Summary of experimental results

Each script applies one method only, making it straightforward to evaluate improvements over the baseline.

â¸»

ğŸ§° Requirements
	â€¢	Python 3.8+
	â€¢	PyTorch, torchvision
	â€¢	timm (for tf_efficientnet_b3 in some scripts)
	â€¢	scikit-learn (for F1-score)
	â€¢	tqdm

Installation Example

pip install torch torchvision timm scikit-learn tqdm

GPU with CUDA is recommended for training.

â¸»

ğŸ—‚ Dataset Structure

All scripts assume an ImageFolder dataset structure:

Dataset/
â”œâ”€ Train/
â”‚  â”œâ”€ fake/  ...
â”‚  â””â”€ real/  ...
â””â”€ Validation/
   â”œâ”€ fake/  ...
   â””â”€ real/  ...

In each script, dataset paths are hardcoded (e.g., /kaggle/input/deepfake-and-real-images/Dataset/...).
You may:
	1.	Modify train_dir, val_dir, save_path variables directly, or
	2.	Create symbolic links matching the expected paths.

â¸»

âš™ï¸ Common Settings
	â€¢	Input size: 300Ã—300
	â€¢	Preprocessing: Resize â†’ ToTensor â†’ Normalize([0.485, 0.456, 0.406],[0.229,0.224,0.225])
	â€¢	Batch size: 32
	â€¢	Optimizer: AdamW(lr=1e-4, weight_decay=1e-4) (may vary per script)
	â€¢	Metrics: macro F1, accuracy
	â€¢	Model saving: triggered when validation F1-score improves

Note: Scripts do not include random seed fixing. For reproducibility, set torch.manual_seed, numpy.random_seed, and cudnn.deterministic manually.

â¸»

ğŸš€ How to Run (Examples)

Each script is independent. Adjust dataset paths, then run:

1) Baseline

python BASELINE/baseline.py

	â€¢	Backbone: torchvision.models.efficientnet_b3
	â€¢	Classifier head: replaced with nn.Linear(in_features, 2)

2) AutoAugment

python autoaugement/autoaugement.py
